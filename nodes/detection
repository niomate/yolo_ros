#!/usr/bin/env python3
import math
import os
import rospy
from ultralytics import YOLO
from sensor_msgs.msg import Image as ImageMsg
import ros_numpy as rosnp
import numpy as np
import supervision as sv
from ultralytics.utils.ops import xyxyxyxy2xywhr
from scipy.spatial.transform import Rotation as R
from scipy.ndimage import rotate

from scipy.cluster.vq import kmeans2
import cv2
from PIL import ImageOps, Image, ImageFilter

from yolo_ros.msg import DetectedObject, DetectedState


def auto_gamma_correction(image):
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    hue, sat, val = cv2.split(hsv)

    # compute gamma = log(mid*255)/log(mean)
    mid = 0.5
    mean = np.mean(val)
    gamma = math.log(mid * 255) / math.log(mean)
    # their adjusted gamma values
    invGamma = 1.0 / gamma

    table = np.array(
        [((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]
    ).astype("uint8")
    # apply gamma correction using the lookup table
    return cv2.LUT(image, table)


def img_callback(msg, args):
    state_pub, annotated_pub, confidence_threshold = args
    # Discard alpha channel
    image = Image.fromarray(rosnp.numpify(msg)[:, :, :3])
    image = ImageOps.autocontrast(image, 0.2)
    image = image.filter(ImageFilter.GaussianBlur(0.2))
    image = np.array(image)
    image = auto_gamma_correction(image)

    encoding = msg.encoding

    result = model(image, verbose=False)[0]
    detections = sv.Detections.from_ultralytics(result).with_nms()
    detections = detections[detections.confidence > confidence_threshold]

    state_msg = DetectedState()
    state_msg.image = msg
    state_msg.detected_objects = []
    annotation_labels = []
    valid_detections = list(range(len(detections)))

    for i, (bbox, _, confidence, _, _, data) in enumerate(detections):
        x1, y1, x2, y2 = bbox.astype(np.int32)
        bbox = np.array(data["xyxyxyxy"]).reshape((-1, 2))
        center = np.mean(bbox).astype(np.int32).tolist()

        # Extract the pixels from the oriented bounding box for a more
        # accurate dominant color estimation
        angle = xyxyxyxy2xywhr(bbox)[-1]
        rotation_matrix = R.from_rotvec([0, 0, angle]).as_matrix()[:2, :2]

        # Rotate oriented bounding box around its center point
        obb = ((bbox - center) @ np.linalg.inv(rotation_matrix)) + center
        obb = obb.astype(np.int32).flatten()

        # Rotate the image in the same direction, so we can
        # retrieve the data more efficiently
        rotated_image = rotate(image, -angle, reshape=False)
        bbox_pixels = rotated_image[obb[1] : obb[7] + 1, obb[0] : obb[6] + 1].reshape(
            (-1, 3)
        )

        if len(bbox_pixels) == 0:
            valid_detections.pop(valid_detections.index(i))
        msg = DetectedObject()
        msg.objclass = str(data["class_name"])
        msg.confidence = confidence
        msg.xyxyxyxy = bbox.reshape((8,)).astype(np.int32).tolist()
        msg.center = center
        msg.id = msg.objclass + "_" + str(hash(str(msg.center)))
            continue

        centroids, labels = kmeans2(bbox_pixels / 255.0, 3)
        _, counts = np.unique(labels, return_counts=True)

        dominant_colors = centroids[np.argsort(-counts)]
        dominant_colors = (dominant_colors * 255).astype(np.uint8)

        msg = DetectedObject()
        msg.objclass = str(data["class_name"])
        msg.confidence = confidence
        msg.xyxyxyxy = bbox.reshape((8,)).astype(np.int32).tolist()
        msg.center = center
        msg.id = msg.objclass + "_" + str(hash(str(msg.center)))
        msg.dominant_color = dominant_colors[0, ::-1].tolist()

        state_msg.detected_objects.append(msg)

        annotation_labels.append(
            f"{msg.dominant_color} {msg.id} @ ({confidence:.2f}) ({msg.center})"
        )

    detections = detections[valid_detections]
    box_annotator = sv.OrientedBoxAnnotator()
    annotated_image = box_annotator.annotate(scene=image.copy(), detections=detections)

    label_annotator = sv.RichLabelAnnotator(text_position=sv.Position.BOTTOM_CENTER)
    annotated_image = label_annotator.annotate(
        scene=annotated_image.copy(), detections=detections, labels=annotation_labels
    )

    encoding = encoding.replace("a", "")

    annotated_pub.publish(
        rosnp.msgify(ImageMsg, np.array(annotated_image), encoding=encoding)
    )

    state_pub.publish(state_msg)


here = os.path.dirname(__file__)

# model_path = os.path.join(here, "../yolov8_obb_finetuned.pt")
model_path = os.path.join(here, "../best.pt")
model = YOLO(model_path)

rospy.init_node("yolo_detection")

det_image_pub = rospy.Publisher("rgb/annotated", ImageMsg, queue_size=5)
det_image_pub_wrist = rospy.Publisher("wrist_rgb/annotated", ImageMsg, queue_size=5)

state_pub = rospy.Publisher("detected_objects", DetectedState, queue_size=5)
state_pub_wrist = rospy.Publisher("detected_objects_wrist", DetectedState, queue_size=5)

rospy.Subscriber(
    "rgb",
    ImageMsg,
    img_callback,
    (state_pub, det_image_pub, 0.7),
)
rospy.Subscriber(
    "wrist_rgb",
    ImageMsg,
    img_callback,
    (state_pub_wrist, det_image_pub_wrist, 0.9),
)


rospy.spin()
