#!/usr/bin/env python3
import math
import os
from collections import defaultdict

import cv2
import numpy as np
import ros_numpy as rosnp
import rospy
import supervision as sv
from scipy.cluster.vq import kmeans2
from scipy.ndimage import rotate
from scipy.spatial.transform import Rotation as R
from sensor_msgs.msg import Image as ImageMsg
from ultralytics import YOLO
from ultralytics.utils.ops import xyxyxyxy2xywhr

from yolo_ros.msg import DetectedObject, DetectedState


def auto_gamma_correction(image):
    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    hue, sat, val = cv2.split(hsv)

    # compute gamma = log(mid*255)/log(mean)
    mid = 0.5
    mean = np.mean(val)
    gamma = math.log(mid * 255) / math.log(mean)
    # their adjusted gamma values
    invGamma = 1.0 / gamma

    table = np.array(
        [((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]
    ).astype("uint8")
    # apply gamma correction using the lookup table
    return cv2.LUT(image, table)


def get_image_slice(image, box):
    if isinstance(box, list):
        box = np.array(box)

    if box.shape == (4, 2):
        p1, *_, p4 = box.astype(np.int32)

    if box.shape == (2, 2):
        p1, p4 = box.astype(np.int32)

    return image[p1[1] : p4[1], p1[0] : p4[0]]


def get_enclosing_rect(box):
    top_left = np.min(box, axis=0)
    bottom_right = np.max(box, axis=0)
    return [top_left, bottom_right]


def rotate_box(box, angle):
    center = np.mean(box, axis=0)
    rotation = R.from_rotvec([0, 0, angle], degrees=True).as_matrix()[:2, :2]
    return ((box - center) @ rotation) + center


def get_obb_pixels(image, obb):
    # Extract the pixels from the oriented bounding box for a more
    # accurate dominant color estimation
    xywhr = xyxyxyxy2xywhr(obb.reshape((1, 8)))[0]
    angle = np.degrees(xywhr[-1]) - 90

    enclosure = get_enclosing_rect(obb)

    local_image = get_image_slice(image, enclosure)
    local_obb = obb - enclosure[0]

    local_obb_rotated = rotate_box(local_obb, angle)
    local_image_rotated = rotate(local_image, angle, reshape=False)

    # Need the enclosing rectangle here just to make sure, the corners
    # are ordered properly.
    # The BB should be nearly axis-aligned anyway, so it shouldn't impact
    # the data we get.
    bbox_pixels = get_image_slice(
        local_image_rotated, get_enclosing_rect(local_obb_rotated)
    )

    return bbox_pixels.reshape((-1, 3))


def compute_dominant_color(pixels):
    centroids, labels = kmeans2(pixels / 255.0, 3)
    _, counts = np.unique(labels, return_counts=True)

    dominant_colors = centroids[np.argsort(-counts)]
    dominant_colors = (dominant_colors * 255).astype(np.uint8)
    return dominant_colors[0, ::-1].tolist()


def img_callback(msg, args):
    state_pub, annotated_pub, confidence_threshold = args

    # Discard alpha channel
    image = rosnp.numpify(msg)[:, :, :3]

    result = model(image, verbose=False)[0]
    detections = sv.Detections.from_ultralytics(result).with_nms()
    detections = detections[detections.confidence > confidence_threshold]

    state_msg = DetectedState()
    state_msg.image = msg
    state_msg.detected_objects = []
    annotation_labels = []
    valid_detections = list(range(len(detections)))
    objects = defaultdict(int)

    for i, (bbox, _, confidence, _, _, data) in enumerate(detections):
        bbox = np.array(data["xyxyxyxy"])
        center = np.mean(bbox, axis=0)

        bbox_pixels = get_obb_pixels(image, bbox)

        if len(bbox_pixels) == 0:
            print("invalid")
            valid_detections.pop(valid_detections.index(i))
            continue

        objclass = str(data["class_name"])
        center = center.astype(np.int32).tolist()
        dominant_color = compute_dominant_color(bbox_pixels)

        objmsg = DetectedObject(
            objclass=objclass,
            confidence=confidence,
            xyxyxyxy=bbox.reshape((8,)).astype(np.int32).tolist(),
            center=center,
            id=f"{objclass}_{objects[objclass]}",
            dominant_color=dominant_color,
        )

        objects[objclass] += 1

        state_msg.detected_objects.append(objmsg)

        annotation_labels.append(
            # f"{objmsg.dominant_color} {objmsg.id} @ ({confidence:.2f}) ({objmsg.center})"
            objmsg.id
        )

    detections = detections[valid_detections]
    box_annotator = sv.OrientedBoxAnnotator()
    annotated_image = box_annotator.annotate(scene=image.copy(), detections=detections)

    label_annotator = sv.RichLabelAnnotator(text_position=sv.Position.BOTTOM_CENTER)
    annotated_image = label_annotator.annotate(
        scene=annotated_image.copy(), detections=detections, labels=annotation_labels
    )

    annotated_pub.publish(
        rosnp.msgify(
            ImageMsg,
            np.array(annotated_image),
            # We discarded the alpha channel, so we don't need to represent this in the encoding
            encoding=msg.encoding.replace("a", ""),
        )
    )

    state_pub.publish(state_msg)


here = os.path.dirname(__file__)

# model_path = os.path.join(here, "../yolov8_obb_finetuned.pt")
model_path = os.path.join(here, "../best.pt")
model = YOLO(model_path)

rospy.init_node("yolo_detection")

det_image_pub = rospy.Publisher("rgb/annotated", ImageMsg, queue_size=5)
det_image_pub_wrist = rospy.Publisher("wrist_rgb/annotated", ImageMsg, queue_size=5)

state_pub = rospy.Publisher("detected_objects", DetectedState, queue_size=5)
state_pub_wrist = rospy.Publisher("detected_objects_wrist", DetectedState, queue_size=5)

rospy.Subscriber(
    "rgb",
    ImageMsg,
    img_callback,
    (state_pub, det_image_pub, 0.7),
)

rospy.Subscriber(
    "wrist_rgb",
    ImageMsg,
    img_callback,
    (state_pub_wrist, det_image_pub_wrist, 0.9),
)


rospy.spin()
